<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>Bo Han</title>
    <base href="https://bhanML.github.io/research.html">
</head>

<body>
<h1 style="padding-left: 0.5em">Bo Han</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="index.html" class="current">Home</a></div>
	<div class="menu-item"><a href="news.html">News</a></div>
    <div class="menu-item"><a href="research.html">Research</a></div>
	<div class="menu-item"><a href="group.html">Research Group</a></div>
	<div class="menu-item"><a href="service.html">Professional Service</a></div>
    <div class="menu-item"><a href="teaching.html">Teaching</a></div>
    <div class="menu-item"><a href="codedata.html">Codes & Data</a></div>
</td>
<td id="layout-content">

<div>
        <h2><a name="research_ecs"></a>RGC Early CAREER Scheme</h2> (PI: Dr. Bo Han, Department of Computer Science, Hong Kong Baptist University)<br><br>

<ul>
<h3>Project Award Information</h3>
<li><p>Award Number: <a href="https://cerg1.ugc.edu.hk/cergprod/scrrm00542.jsp?proj_id=22200720&old_proj_id=null&proj_title=&isname=&ioname=&institution=&subject=E2&pages=1&year=2020&theSubmit=22200720" target="_blank">RGC ECS 22200720</a></p></li>
<li><p>Title: Trustworthy Deep Learning from Open-set Corrupted Data</p></li>
<li><p>Principal Investigator (PI): Dr. Bo Han, Department of Computer Science, Hong Kong Baptist University</p></li>
</ul>

<ul>
<h3>Project Summary</h3>
Trustworthy learning from corrupted data is a vital research topic in modern machine learning (i.e., deep learning), since most real-world data are easily imperfect and corrupted, such as financial data, healthcare data and social-network data. However, existing works in trustworthy deep learning (TDL) tend to implicitly assume that corrupted data should be closed-set: samples with corrupted labels own true classes known in the training data; samples with the set of known classes can be crafted as adversarial examples in the testing phase; and samples in source domain share the same class of samples in target domain. Such closed-set assumption is the crux of existing TDL methods, which is too restrictive for many real-world applications. This project aims to address this conundrum by developing models, algorithms and prototype system for trustworthy deep learning from open-set corrupted data. The outcome of the research could significantly robustify TDL techniques in open-world knowledge discovery and decision-making processes such as those in personalized medicine, financial engineering and scientific discoveries.
</ul>

<ul>
<h3>Research Publications</h3>
The following papers focus on open-set reliability:
</ul>
<ul>
<li><p>part-dependent label noise:
towards instance-dependent label noise (<a href="https://papers.nips.cc/paper/2020/file/5607fe8879e4fd269e88387e8cb30b7e-Paper.pdf" target="_blank">NeurIPS'20</a>)</p></li>
<li><p>confidence scores make instance-dependent label-noise learning possible (<a href="https://arxiv.org/pdf/2001.03772.pdf" target="_blank">ICML'21</a>)</p></li>
<li><p>instance-dependent label-noise learning under a structural causal model (<a href="https://arxiv.org/pdf/2109.02986.pdf" target="_blank">NeurIPS'21</a>)</p></li>
<li><p>learning with group noise (<a href="https://arxiv.org/abs/2101.05467.pdf" target="_blank">AAAI'21</a>)</p></li>
<li><p>exploiting class activation value for partial-label learning (<a href="https://openreview.net/pdf?id=qqdXHUGec9h" target="_blank">ICLR'22</a>)</p></li>
<li><p>fair classification with instance-dependent label noise (<a href="https://openreview.net/pdf?id=s-pcpETLpY" target="_blank">CLeaR'22</a>)</p></li>
</ul>

<ul> The following papers focus on open-set robustness:
</ul>
<ul>
<li><p>probabilistic margins for instance reweighting in adversarial training (<a href="https://arxiv.org/pdf/2106.07904.pdf" target="_blank">NeurIPS'21</a>)</p></li>
<li><p>understanding adversarial attacks via maximum mean discrepancy (<a href="https://arxiv.org/pdf/2010.11415.pdf" target="_blank">ICML'21</a>)</p></li>
<li><p>learning diverse-structured networks for adversarial robustness (<a href="https://arxiv.org/pdf/2102.01886.pdf" target="_blank">ICML'21</a>)</p></li>
<li><p>geometry-aware instance-reweighted adversarial training (<a href="https://arxiv.org/pdf/2010.01736.pdf" target="_blank">ICLR'21</a>)</p></li>
<li><p>adversarial robustness through the lens of causality (<a href="https://openreview.net/pdf?id=cZAi1yWpiXQ" target="_blank">ICLR'22</a>)</p></li>
<li><p>understanding and improving graph injection attack by promoting unnoticeability (<a href="https://openreview.net/pdf?id=wkMG8cdvh7-" target="_blank">ICLR'22</a>)</p></li>
</ul>

<ul> The following papers focus on open-set adaptivity:
</ul>
<ul>
<li><p>tohan: a one-step approach towards few-shot hypothesis adaptation (<a href="https://arxiv.org/pdf/2106.06326.pdf" target="_blank">NeurIPS'21</a>)</p></li>
<li><p>universal semi-supervised learning (<a href="https://openreview.net/pdf?id=s-pcpETLpY" target="_blank">NeurIPS'22</a>)</p></li>
<li><p>meta discovery: learning to discover novel classes given very limited data (<a href="https://openreview.net/pdf?id=MEpKGLsY8f" target="_blank">ICLR'22</a>)</p></li>
</ul>

<ul> The following papers focus on automating trustworthy deep learning algorithms:
</ul>
<ul>
<li><p>searching to exploit memorization effect in learning from noisy labels (<a href="https://proceedings.icml.cc/static/paper_files/icml/2020/3285-Paper.pdf" target="_blank">ICML'20</a>)</p></li>
</ul>

<ul>
<h3>Software</h3>
<li><p>part-dependent label noise: towards instance-dependent label noise, [<a href="https://github.com/xiaoboxia/Part-dependent-label-noise" target="_blank">code</a>]</p></li>
<li><p>learning with group noise, [<a href="https://github.com/QizhouWang/Max-Matching" target="_blank">code</a>]</p></li>
<li><p>geometry-aware instance-reweighted adversarial training, [<a href="https://github.com/zjfheart/Geometry-aware-Instance-reweighted-Adversarial-Training" target="_blank">code</a>]</p></li>
<li><p>confidence scores make instance-dependent label-noise learning possible, [<a href="https://github.com/antoninbrthn/CSIDN" target="_blank">code</a>]</p></li>
<li><p>maximum mean discrepancy is aware of adversarial attacks, [<a href="https://github.com/Sjtubrian/SAMMD" target="_blank">code</a>]</p></li>
<li><p>learning diverse-structured networks for adversarial robustness, [<a href="https://github.com/d12306/dsnet" target="_blank">code</a>]</p></li>
<li><p>probabilistic margins for instance reweighting in adversarial training, [<a href="https://github.com/QizhouWang/MAIL" target="_blank">code</a>].</p></li>
<li><p>tohan: a one-step approach towards few-shot hypothesis adaptation, [<a href="https://github.com/Haoang97/TOHAN" target="_blank">code</a>]</p></li>
<li><p>instance-dependent label-noise learning under a structural causal model, [<a href="https://github.com/a5507203/IDLN" target="_blank">code</a>]</p></li>
<li><p>adversarial robustness through the lens of causality, [code]</p></li>
<li><p>exploiting class activation value for partial-label learning, [code]</p></li>
<li><p>understanding and improving graph injection attack by promoting unnoticeability, [code]</p></li>
<li><p>meta discovery: learning to discover novel classes given very limited data, [code]</p></li>
</ul>

<ul>
<h3>Education</h3>
<li><p>UG Course: <a href="https://www.comp.hkbu.edu.hk/v1/file/course/COMP3057.pdf" target="_blank">COMP3057</a> (2021 Autumn), <a href="https://www.comp.hkbu.edu.hk/v1/file/course/COMP4015.pdf" target="_blank">COMP4015</a> (2020 Autumn)</p></li>
<li><p>PG Course: <a href="https://www.comp.hkbu.edu.hk/v1/file/course/COMP7250.pdf" target="_blank">COMP7250</a> (2021 Spring, 2022 Spring), <a href="https://www.comp.hkbu.edu.hk/v1/file/course/COMP7160.pdf" target="_blank">COMP7160</a> (2021 Autumn)</p></li>
<li><p>Tutorial: <a href="https://wsl-workshop.github.io/ijcai21-tutorial.html" target="_blank">IJCAI'21</a> Learning with Noisy Supervision, <a href="https://wsl-workshop.github.io/acml21-tutorial.html" target="_blank">ACML'21</a> Learning under Noisy Supervision</p></li>
<li><p>Undergraduate Research Programme (<a href="https://research.hkbu.edu.hk/study-with-us/undergraduate-research-programme" target="_blank">UGRP</a>): Yifeng Chen, Xinyue Hu, Yixiao Zheng</p></li>
</ul>

<ul>
<h3>Collaborators</h3>
<li><p> University: The University of Tokyo, Carnegie Mellon University, The University of Texas at Austin, The University of Sydney, The University of Melbourne, The Chinese University of Hong Kong, Hong Kong University of Science and Technology, Tsinghua University, Nanjing University of Science and Technology</p></li>
<li><p> Institute: RIKEN Center for Advanced Intelligence Project, Max Planck Institute for Intelligent Systems</p></li>
<li><p>	Industry: Microsoft Research, Alibaba Research </p></li>
</ul>

<ul>
<h3>Acknowlewdgement</h3>
This material is based upon work supported by the the Research Grants Council (RGC) of Hong Kong SAR under Grant No. 22200720. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Research Grants Council (RGC) of Hong Kong SAR.
</ul>

</div>

</td>
</tr>
</table>
</body>
</html>
