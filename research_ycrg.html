<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>Bo Han</title>
    <base href="https://bhanML.github.io/research.html">
</head>

<body>
<h1 style="padding-left: 0.5em">Bo Han</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="index.html">Home</a></div>
	<div class="menu-item"><a href="news.html">News</a></div>
    <div class="menu-item"><a href="research.html">Research</a></div>
	<div class="menu-item"><a href="group.html">Research Group</a></div>
	<div class="menu-item"><a href="service.html">Professional Service</a></div>
    <div class="menu-item"><a href="teaching.html">Teaching</a></div>
    <div class="menu-item"><a href="codedata.html">Codes & Data</a></div>
	<div class="menu-item"><a href="awards.html">Awards & Honors</a></div>
</td>
<td id="layout-content">

<div>
        <h2><a name="research_ecs"></a>RGC Young Collaborative Research Grant</h2> (PC: Prof. Bo Han, Department of Computer Science, Hong Kong Baptist University)<br><br>

<ul>
<h3>Project Award Information</h3>
<li><p>Award Number: <a href="https://www.ugc.edu.hk/eng/rgc/funding_opport/crf/funded%20research/crf24_lay_sum.html#C2005-24Y" target="_blank">RGC YCRG C2005-24Y</a></p></li>
<li><p>Title: Towards Trustworthy Foundation Models under Imperfect Scenarios</p></li>
<li><p>Principal Investigator (PC): Prof. Bo Han, Department of Computer Science, Hong Kong Baptist University</p></li>
</ul>

<ul>
<h3>Project Summary</h3>
We have entered a new age of artificial intelligence, since Foundation Models (FMs) like ChatGPT and Sora emerge as pivotal tools with great capabilities in a broad range of domains and tasks. However, the deployment of FMs has surfaced critical concerns, particularly in robustness, safety, fairness, and reliability. In social science, while FMs offer advanced analysis of extensive qualitative data sets, they also face the problem of ensuring robustness against data anomalies and fairness in representation. In medical sciences, FMs promise a revolution through their ability to process large-scale medical datasets, yet they must do so with utmost safety and reliability to prevent harmful outcomes. Therefore, this project introduces solutions to the issues of FMs by developing trustworthy FMs. Specifically, trustworthy FMs will address the four grand challenges, including robustness against noisy inputs, safety against adversarial prompts, fairness against biased training data, and reliability against insufficient knowledge. Moreover, by developing advanced and targeted solutions, this project aims to bolster the functionality and dependability of trustworthy FMs, particularly within the critical spheres of social and medical sciences, thereby facilitating their responsible and beneficial integration into these fields. In summary, this collaborative project is expected to address the four grand challenges and construct trustworthy FMs, which can be further deployed to broader scientific and industrial applications.
</ul>

<ul>
<h3>Research Publications</h3>
The following papers focus on robustness against noisy inputs: 
</ul>
<ul>
<li><p>noisy test-time adaptation in vision-language models (<a href="https://openreview.net/pdf?id=iylpeTI0Ql" target="_blank">ICLR'25</a>)</p></li>
<li><p>active reasoning benchmark (<a href="https://openreview.net/pdf?id=LCaTpVuvpj" target="_blank">ICML'25</a>)</p></li>
<li><p>belief-driven multi-agent LLM reasoning (<a href="https://openreview.net/pdf?id=RQwexjUCxm" target="_blank">ICML'25</a>)</p></li>
<li><p>learning to instruct for visual instruction tuning (<a href="https://openreview.net/pdf?id=NQSWkmjODD" target="_blank">NeurIPS'25</a>)</p></li>
<li><p>multi-agent debate with memory masking (<a href="https://openreview.net/pdf?id=EdTt8nMAMA" target="_blank">ICLR'26</a>)</p></li>
<li><p>stable self-supervised RL for LLMs reasoning (<a href="https://openreview.net/pdf?id=fDk95XPsCU" target="_blank">ICLR'26</a>)</p></li>
<li><p>visualizing the reasoning process of large language models (<a href="https://openreview.net/pdf?id=XpoQ812d0A" target="_blank">ICLR'26</a>)</p></li>
</ul>

<ul> The following papers focus on safety against adversarial prompts:
</ul>
<ul>
<li><p>understanding and enhancing the transferability of jailbreaking attacks (<a href="https://openreview.net/pdf?id=asR9FVd4eL" target="_blank">ICLR'25</a>)</p></li>
<li><p>effective evaluations and comparisons for LLM unlearning (<a href="https://openreview.net/pdf?id=wUtCieKuQU" target="_blank">ICLR'25</a>)</p></li>
<li><p>exploring criteria of loss reweighting to enhance LLM unlearning (<a href="https://openreview.net/pdf?id=mGOugCZlAq" target="_blank">ICML'25</a>)</p></li>
<li><p>ensuring jailbreak defense via answer-then-check (<a href="https://openreview.net/pdf?id=DK6AToxJNo" target="_blank">ICLR'26</a>)</p></li>
<li><p>your downloaded LoRA from sharing platforms might be unsafe (<a href="https://openreview.net/pdf?id=4YgvVRoSnF" target="_blank">ICLR'26</a>)</p></li>
<li><p>LLM unlearning with LLM beliefs (<a href="https://openreview.net/pdf?id=qCfYOLAzti" target="_blank">ICLR'26</a>)</p></li>
</ul>

<ul> The following papers focus on fairness against biased training data:
</ul>
<ul>
<li><p>interpretability through the lens of semantic dependency (<a href="https://openreview.net/pdf?id=7v2loOdcLH" target="_blank">ICML'25</a>)</p></li>
<li><p>conditional independence test (<a href="https://openreview.net/pdf?id=R1CximX3Cw" target="_blank">NeurIPS'25</a>)</p></li>
<li><p>a robust method to discover causal or anticausal relation (<a href="https://openreview.net/pdf?id=Q0s6kgrUMr" target="_blank">ICLR'25</a>)</p></li>
<li><p>on the thinking-language modeling gap in large language models (<a href="https://openreview.net/pdf?id=lSWIzMX2Ie" target="_blank">ICLR'26</a>)</p></li>
</ul>

<ul> The following papers focus on reliability against insufficient knowledge:
</ul>
<ul>
<li><p>advancing machine-generated text detection (<a href="https://openreview.net/pdf?id=BUkXhMb7ml" target="_blank">NeurIPS'25</a>)</p></li>
<li><p>detecting generated images by fitting natural image distributions (<a href="https://openreview.net/pdf?id=27xTIAFbc6" target="_blank">NeurIPS'25</a>)</p></li>
<li><p>understanding valuable preference data for LLM alignment (<a href="https://openreview.net/pdf?id=FUp0KeEEBs" target="_blank">ICLR'26</a>)</p></li>
<li><p>task-aware data selection for LLM finetuning (<a href="https://openreview.net/pdf?id=R40WoYbYab" target="_blank">ICLR'26</a>)</p></li>
<li><p>markov-informed calibration for boosting machine-generated text detection (<a href="https://openreview.net/pdf?id=Lzwkeg2o2z" target="_blank">ICLR'26</a>)</p></li>
</ul>

<ul>
<h3>Software</h3>
<li><p>noisy test-time adaptation in vision-language models, [<a href="https://github.com/tmlr-group/ZS-NTTA" target="_blank">code</a>]</p></li>
<li><p>active reasoning benchmark, [<a href="https://github.com/tmlr-group/AR-Bench" target="_blank">code</a>]</p></li>
<li><p>belief-driven multi-agent LLM reasoning, [<a href="https://github.com/tmlr-group/ECON" target="_blank">code</a>]</p></li>
<li><p>learning to instruct for visual instruction tuning, [<a href="https://github.com/Feng-Hong/L2T" target="_blank">code</a>]</p></li>
<li><p>multi-agent debate with memory masking, [code]</p></li>
<li><p>stable self-supervised RL for LLMs reasoning, [code]</p></li>
<li><p>visualizing the reasoning process of large language models, [code]</p></li>
<li><p>understanding and enhancing the transferability of jailbreaking attacks, [<a href="https://github.com/tmllab/2025_ICLR_PiF" target="_blank">code</a>]</p></li>
<li><p>effective evaluations and comparisons for LLM unlearning, [<a href="https://github.com/tmlr-group/Unlearning-with-Control" target="_blank">code</a>]</p></li>
<li><p>exploring criteria of loss reweighting to enhance LLM unlearning, [<a href="https://github.com/tmlr-group/SatImp" target="_blank">code</a>]</p></li>
<li><p>ensuring jailbreak defense via answer-then-check, [code]</p></li>
<li><p>your downloaded LoRA from sharing platforms might be unsafe, [code]</p></li>
<li><p>LLM unlearning with LLM beliefs, [code]</p></li>
<li><p>interpretability through the lens of semantic dependency, [code]</p></li>
<li><p>conditional independence test, [<a href="https://github.com/wenjiewang3/PowerKCI" target="_blank">code</a>]</p></li>
<li><p>a robust method to discover causal or anticausal relation, [code]</p></li>
<li><p>on the thinking-language modeling gap in large language models, [code]</p></li>
<li><p>advancing machine-generated text detection, [<a href="https://github.com/tmlr-group/Easy2Hard" target="_blank">code</a>]</p></li>
<li><p>detecting generated images by fitting natural image distributions, [<a href="https://github.com/tmlr-group/ConV" target="_blank">code</a>]</p></li>
<li><p>understanding valuable preference data for LLM alignment, [code]</p></li>
<li><p>task-aware data selection for LLM finetuning, [code]</p></li>
<li><p>markov-informed calibration for boosting machine-generated text detection, [code]</p></li>
</ul>

<ul>
<h3>System</h3>
<li><p>AlphaApollo, [<a href="https://arxiv.org/pdf/2510.06261" target="_blank">paper</a>] [<a href="https://github.com/tmlr-group/AlphaApollo" target="_blank">code</a>] [<a href="https://alphaapollo.org/" target="_blank">project</a>]</p></li>
</ul>

<ul>
<h3>Education</h3>
<li><p>UG Course: <a href="https://www.comp.hkbu.edu.hk/v1/file/course/COMP3065.pdf" target="_blank">COMP3065</a> (2026 Spring)</p></li>
<li><p>PG Course: <a href="https://www.comp.hkbu.edu.hk/v1/file/course/COMP7250.pdf" target="_blank">COMP7250</a> (2025 Spring, 2026 Spring)</p></li>
<li><p>Tutorial: <a href="https://aaai.org/conference/aaai/aaai-26/tutorial-and-lab-list/#th10" target="_blank">AAAI'26</a> Trustworthy Machine Reasoning with Foundation Models, <a href="https://aaai.org/conference/aaai/aaai-26/tutorial-and-lab-list/#th19" target="_blank">AAAI'26</a> When AI "Forgets" for Good: The Science and Practice of Machine Unlearning for AI Safety, <a href="https://aaai.org/conference/aaai/aaai-26/tutorial-and-lab-list/#th03" target="_blank">AAAI'26</a> Handling Out-of-Distribution Data in the Open World: Principles and Practice for Reliable AI</p></li>
<li><p>Lecture: <a href="https://deeplearn.irdta.eu/2026/blog/speakers/bo-han/" target="_blank">DeepLearn 2026</a> Trustworthy Machine Learning from Data to Models, <a href="https://essai2026.eu/courses.php#id-16" target="_blank">ESSAI 2026</a> Trustworthy Machine Learning from Data to Models, <a href="https://www.i.u-tokyo.ac.jp/edu/proced/sche_sa_e.shtml" target="_blank">UTokyo 2026 Guest Lecture</a> Trustworthy Foundation Models</p></li>
</ul>

<ul>
<h3>Collaborators</h3>
<li><p> University: Stanford University, Carnegie Mellon University, The University of Texas at Austin, University of California San Diego, University of California Santa Cruz, Université de Montréal, HEC Montréal, The University of Sydney, The University of Melbourne, University of Technology Sydney, The University of Tokyo, Mohamed bin Zayed University of Artificial Intelligence, The Chinese University of Hong Kong, Hong Kong University of Science and Technology, The Hong Kong Polytechnic University, The Hong Kong University of Science and Technology (GuangZhou), University of Macau, Shanghai Jiao Tong University, Fudan University, University of Science and Technology of China, South China University of Technology, Northeastern University, Hefei University of Technology</p></li>
<li><p> Institute: RIKEN Center for Advanced Intelligence Project, Mila Québec AI Institute</p></li>
<li><p>	Industry: ByteDance Seed, Tencent WeChat, Microsoft Research, Intel AI Lab</p></li>
</ul>

<ul>
<h3>Acknowlewdgement</h3>
This material is based upon work supported by the RGC under Grant No. C2005-24Y. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the RGC.
</ul>

</div>

</td>
</tr>
</table>
</body>
</html>
