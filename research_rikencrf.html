<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>Bo Han</title>
    <base href="https://bhanML.github.io/research.html">
</head>

<body>
<h1 style="padding-left: 0.5em">Bo Han</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="index.html">Home</a></div>
	<div class="menu-item"><a href="news.html">News</a></div>
    <div class="menu-item"><a href="research.html">Research</a></div>
	<div class="menu-item"><a href="group.html">Research Group</a></div>
	<div class="menu-item"><a href="service.html">Professional Service</a></div>
    <div class="menu-item"><a href="teaching.html">Teaching</a></div>
    <div class="menu-item"><a href="codedata.html">Codes & Data</a></div>
	<div class="menu-item"><a href="awards.html">Awards & Honors</a></div>
</td>
<td id="layout-content">

<div>
        <h2><a name="research_ecs"></a>RIKEN Collaborative Research Fund</h2> (PI: Dr. Bo Han, Department of Computer Science, Hong Kong Baptist University & RIKEN Center for Advanced Intelligence Project)<br><br>

<ul>
<h3>Project Award Information</h3>
<li><p>Title: New Directions in Trustworthy Machine Learning</p></li>
<li><p>Principal Investigator (PI): Dr. Bo Han, Department of Computer Science, Hong Kong Baptist University & RIKEN Center for Advanced Intelligence Project</p></li>
</ul>

<ul>
<h3>Project Summary</h3>
Trustworthy machine learning (TML) under imperfect data has recently brought much attention in the data-centric fields of machine learning (ML) and artificial intelligence (AI). Specifically, there are mainly three types of imperfect data along with their challenges for ML, including i) label-level imperfection: noisy labels; ii) feature-level imperfection: adversarial examples; iii) distribution-level imperfection: out-of-distribution data. Therefore, in this collaborative project, we systematically investigate three types of imperfect data in the age of deep learning, and propose our solutions with academic partners in RIKEN AIP. More importantly, this project aims to explore new directions in trustworthy machine learning, such as trustworthy foundation models, trustworthy federated learning, and trustworthy causal learning.
</ul>

<ul>
<h3>Research Publications</h3>
<li><p>forgetting may make learning with noisy labels more robust (<a href="https://proceedings.mlr.press/v119/han20c/han20c.pdf" target="_blank">ICML'20</a>)</p></li>
<li><p>confidence scores make instance-dependent label-noise learning possible (<a href="http://proceedings.mlr.press/v139/berthon21a/berthon21a.pdf" target="_blank">ICML'21</a>, <font color="red">Long Oral</font>)</p></li>
<li><p>maximum mean discrepancy test is aware of adversarial attacks (<a href="https://proceedings.mlr.press/v139/gao21b/gao21b.pdf" target="_blank">ICML'21</a>)</p></li>
<li><p>adversarial training with complementary labels (<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/959f70ee50044bed305e48e3484005a7-Paper-Conference.pdf" target="_blank">NeurIPS'22</a>, <font color="red">Spotlight</font>)</p></li>
<li><p>collaborate to improve adversarial robustness (<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/d16152d53088ad779ffa634e7bf66166-Paper-Conference.pdf" target="_blank">NeurIPS'22</a>)</p></li>
<li><p>learning to discover novel classes given very limited data (<a href="https://openreview.net/pdf?id=MEpKGLsY8f" target="_blank">ICLR'22</a>, <font color="red">Spotlight</font>)</p></li>
<li><p>exploiting class activation value for partial-label learning (<a href="https://openreview.net/pdf?id=qqdXHUGec9h" target="_blank">ICLR'22</a>)</p></li>
<li><p>diversity-enhancing generative network for few-shot hypothesis adaptation (<a href="https://proceedings.mlr.press/v202/dong23d/dong23d.pdf" target="_blank">ICML'23</a>)</p></li>
<li><p>diversified outlier exposure for out-of-distribution detection (<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/46d943bc6a15a57c923829efc0db7c7a-Paper-Conference.pdf" target="_blank">NeurIPS'23</a>)</p></li>
<li><p>what if the input is expanded in OOD detection (<a href="https://openreview.net/pdf?id=XfPiFRnuAS" target="_blank">NeurIPS'24</a>)</p></li>
<li><p>self-calibrated tuning of vision-language models for out-of-distribution detection (<a href="https://openreview.net/pdf?id=w6vbfSC1y0" target="_blank">NeurIPS'24</a>)</p></li>
<li><p>balancing similarity and complementarity for unimodal and multimodal federated learning (<a href="https://openreview.net/pdf?id=v6tAdeCXKH" target="_blank">ICML'24</a>)</p></li>
<li><p>accurate forgetting for heterogeneous federated continual learning (<a href="https://openreview.net/pdf?id=ShQrnAsbPI" target="_blank">ICLR'24</a>)</p></li>
<li><p>decoupling the class label and the target concept in machine unlearning (<a href="https://arxiv.org/pdf/2406.08288" target="_blank">arXiv'24</a>)</p></li>
<li><p>towards effective evaluations and comparison for LLM unlearning methods (<a href="https://openreview.net/pdf?id=wUtCieKuQU" target="_blank">ICLR'25</a>)</p></li>
</ul>

<ul>
<h3>Software</h3>
<li><p>forgetting may make learning with noisy labels more robust, [<a href="https://github.com/bhanML/SIGUA" target="_blank">code</a>]</p></li>
<li><p>confidence scores make instance-dependent label-noise learning possible, [<a href="https://github.com/tmlr-group/CSIDN" target="_blank">code</a>]</p></li>
<li><p>maximum mean discrepancy test is aware of adversarial attacks, [<a href="https://github.com/tmlr-group/SAMMD" target="_blank">code</a>]</p></li>
<li><p>adversarial training with complementary labels, [<a href="https://github.com/tmlr-group/ATCL" target="_blank">code</a>]</p></li>
<li><p>collaborate to improve adversarial robustness, [<a href="https://github.com/cuis15/synergy-of-experts" target="_blank">code</a>]</p></li>
<li><p>learning to discover novel classes given very limited data, [<a href="https://github.com/tmlr-group/MEDI" target="_blank">code</a>]</p></li>
<li><p>exploiting class activation value for partial-label learning, [<a href="https://github.com/tmlr-group/CAVL" target="_blank">code</a>]</p></li>
<li><p>diversity-enhancing generative network for few-shot hypothesis adaptation, [<a href="https://github.com/tmlr-group/DEG-Net" target="_blank">code</a>]</p></li>
<li><p>diversified outlier exposure for out-of-distribution detection, [<a href="https://github.com/tmlr-group/DivOE" target="_blank">code</a>]</p></li>
<li><p>what if the input is expanded in OOD detection, [<a href="https://github.com/tmlr-group/CoVer" target="_blank">code</a>]</p></li>
<li><p>self-calibrated tuning of vision-language models for out-of-distribution detection, [<a href="https://github.com/tmlr-group/SCT" target="_blank">code</a>]</p></li>
<li><p>balancing similarity and complementarity for unimodal and multimodal federated learning, [<a href="https://github.com/yankd22/FedSaC/" target="_blank">code</a>]</p></li>
<li><p>accurate forgetting for heterogeneous federated continual learning, [<a href="https://github.com/zaocan666/AF-FCL" target="_blank">code</a>]</p></li>
<li><p>decoupling the class label and the target concept in machine unlearning, [code]</p></li>
<li><p>towards effective evaluations and comparison for LLM unlearning methods, [code]</p></li>
</ul>

<ul>
<h3>Collaborators</h3>
<li><p> Institute: RIKEN Center for Advanced Intelligence Project</p></li>
</ul>

<ul>
<h3>Acknowlewdgement</h3>
This material is based upon work supported by the RIKEN AIP. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the RIKEN AIP.
</ul>

</div>

</td>
</tr>
</table>
</body>
</html>
