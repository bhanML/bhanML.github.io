<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>Bo Han</title>
    <base href="https://bhanML.github.io/research.html">
</head>

<body>
<h1 style="padding-left: 0.5em">Bo Han</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="index.html" class="current">Home</a></div>
	<div class="menu-item"><a href="news.html">News</a></div>
    <div class="menu-item"><a href="research.html">Research</a></div>
	<div class="menu-item"><a href="group.html">Research Group</a></div>
	<div class="menu-item"><a href="service.html">Professional Service</a></div>
    <div class="menu-item"><a href="teaching.html">Teaching</a></div>
    <div class="menu-item"><a href="codedata.html">Codes & Data</a></div>
</td>
<td id="layout-content">

<div>
        <h2><a name="research"></a>Research (Selected Topics)</h2> (* Link to: RGC Early CAREER Scheme website)<br>
<ul>
<h3>Weakly Supervised Representation Learning</h3>
Modern machine learning is migrating to the era of complex models (e.g., deep neural networks), which emphasizes the data representation highly. This learning paradigm is known as representation learning. It is noted that representation learning normally requires a plethora of well-annotated data. Nonetheless, for startups or non-profit organizations, such data is barely acquirable due to the cost of labeling data or the intrinsic scarcity in the given domain. These practical issues motivate us to research and pay attention to weakly supervised representation learning (WSRL), since WSRL does not require such a huge amount of annotated data. Over the years, we have developed techniques for weakly supervised representation learning, such as label-noise representation learning and wildly transferable representation learning.
</ul>

<ul>
Relevant Work/Publications:
</ul>
<ul>
<li><p>machine learning with noisy labels (<a href="https://mitpress.mit.edu/books/series/adaptive-computation-and-machine-learning-series" target="_blank">Monograph</a>, accepted and under preparation)</p></li>
<li><p>label-noise representation learning (<a href="https://arxiv.org/pdf/2011.04406.pdf" target="_blank">Survey</a>)</p></li>
<li><p>training deep networks via memorization effects (<a href="https://papers.nips.cc/paper/8072-co-teaching-robust-training-of-deep-neural-networks-with-extremely-noisy-labels" target="_blank">NeurIPS'18</a>, <a href="http://proceedings.mlr.press/v97/yu19b.html" target="_blank">ICML'19</a>, <a href="https://openreview.net/forum?id=Eql5b1_hTE4" target="_blank">ICLR'21</a>, <a href="https://arxiv.org/pdf/2106.00445.pdf" target="_blank">NeurIPS'21</a>, <a href="https://arxiv.org/pdf/2106.15853.pdf" target="_blank">NeurIPS'21</a>, <a href="https://openreview.net/pdf?id=xENf4QUL4LW" target="_blank">ICLR'22</a>)</p></li>
<li><p>estimating the noise transition matrix (<a href="https://papers.nips.cc/paper/7825-masking-a-new-perspective-of-noisy-supervision" target="_blank">NeurIPS'18</a>, <a href="https://papers.nips.cc/paper/2019/file/9308b0d6e5898366a4a986bc33f3d3e7-Paper.pdf" target="_blank">NeurIPS'19</a>, <a href="https://papers.nips.cc/paper/2020/file/512c5cad6c37edb98ae91c8a76c3a291-Paper.pdf" target="_blank">NeurIPS'20</a>, <a href="https://arxiv.org/pdf/2102.02400.pdf" target="_blank">ICML'21</a>)</p></li>
<li><p>stochastic integrated gradient underweighted ascent (<a href="https://proceedings.mlr.press/v119/han20c/han20c.pdf" target="_blank">ICML'20</a>)</p></li>
<li><p>instance-dependent label-noise or
positive-unlabeled learning (<a href="https://papers.nips.cc/paper/2020/file/5607fe8879e4fd269e88387e8cb30b7e-Paper.pdf" target="_blank">NeurIPS'20</a>, <a href="https://arxiv.org/pdf/2001.03772.pdf" target="_blank">ICML'21</a>, <a href="https://arxiv.org/abs/2101.05467.pdf" target="_blank">AAAI'21</a>, <a href="https://www.computer.org/csdl/journal/tp/5555/01/09361303/1rsezbEXNxS" target="_blank">TPAMI'21</a>, <a href="https://arxiv.org/pdf/2109.02986.pdf" target="_blank">NeurIPS'21</a>, <a href="https://openreview.net/pdf?id=s-pcpETLpY" target="_blank">CLeaR'22</a>)</p></li>
<li><p>learning from crowds (<a href="https://link.springer.com/article/10.1007/s10994-017-5674-0" target="_blank">MLJ'17</a>, <a href="https://link.springer.com/article/10.1007/s10994-018-5716-2" target="_blank">MLJ'18</a>, <a href="https://link.springer.com/article/10.1007/s10994-018-5766-5" target="_blank">MLJ'18</a>)</p></li>
<li><p>wildly transferable representation learning (<a href="https://arxiv.org/pdf/1905.07720.pdf" target="_blank">arXiv'19</a>, <a href="https://arxiv.org/pdf/2106.06326.pdf" target="_blank">NeurIPS'21</a>, <a href="https://openreview.net/pdf?id=MEpKGLsY8f" target="_blank">ICLR'22</a>)</p></li>
<li><p>variational imitation learning with diverse-quality demonstrations (<a href="https://arxiv.org/abs/1909.06769" target="_blank">ICML'20</a>)</p></li>
<li><p>learning with open-set labels (<a href="https://papers.nips.cc/paper/2021/file/e06f967fb0d355592be4e7674fa31d26-Paper.pdf" target="_blank">NeurIPS'21</a>)</p></li>
<li><p>learning with weakly supervised labels (<a href="https://arxiv.org/pdf/1912.12927.pdf" target="_blank">ICML'20</a>, <a href="https://arxiv.org/pdf/2007.08929.pdf" target="_blank">NeurIPS'20</a>, <a href="https://www.ijcai.org/proceedings/2020/0361.pdf" target="_blank">IJCAI'20</a>, <a href="https://arxiv.org/pdf/2010.01875.pdf" target="_blank">ICML'21</a>, <a href="https://arxiv.org/pdf/2006.07831.pdf" target="_blank">ICML'21</a>, <a href="https://arxiv.org/pdf/2103.09468.pdf" target="_blank">AAAI'21</a>, <a href="https://openreview.net/pdf?id=aYAA-XHKyk" target="_blank">ICLR'22</a>, <a href="https://openreview.net/pdf?id=qqdXHUGec9h" target="_blank">ICLR'22</a>)</p></li>
</ul>

<ul>
<h3>Security, Privacy and Robustness in Machine Learning</h3>
In this research thrust, I am interested in the following question: How can we preserve the security, privacy and robustness in training complex models? We have investigated learning algorithms for handling large-scale sensitive data safely. One of the key ideas is to bridge private updates of the primal variable with gradual curriculum learning. We have proposed one of the pioneer approaches for investigating the robustness of residual networks from the perspective of dynamic system. Specifically, we exploited the step factor in the Euler method to control the robustness of ResNet in both its  training and generalization. More recently, we derived a series of adversarial learning algorithms, which mainly focus on empirical defense.
</ul>

<ul>
Relevant Work/Publications:
</ul>
<ul>
<li><p>privacy-preserving stochastic gradual learning (<a href="https://ieeexplore.ieee.org/document/8949708" target="_blank">TKDE'19</a>)</p></li>
<li><p>towards robust ResNet: a small step but a giant leap (<a href="https://www.ijcai.org/proceedings/2019/595" target="_blank">IJCAI'19</a>)</p></li>
<li><p>friendly adversarial training via early-stopped PGD (<a href="https://proceedings.mlr.press/v119/zhang20z/zhang20z.pdf" target="_blank">ICML'20</a>)</p></li>
<li><p>geometry-aware instance-reweighted adversarial training (<a href="https://arxiv.org/pdf/2010.01736.pdf" target="_blank">ICLR'21</a>, <a href="https://arxiv.org/pdf/2106.07904.pdf" target="_blank">NeurIPS'21</a>)</p></li>
<li><p>understanding adversarial attacks via maximum mean discrepancy (<a href="https://arxiv.org/pdf/2010.11415.pdf" target="_blank">ICML'21</a>)</p></li>
<li><p>learning diverse-structured networks for adversarial robustness (<a href="https://arxiv.org/pdf/2102.01886.pdf" target="_blank">ICML'21</a>)</p></li>
<li><p>towards defending against adversarial examples via attack-invariant features (<a href="https://arxiv.org/pdf/2106.05036.pdf" target="_blank">ICML'21</a>)</p></li>
<li><p>understanding the interaction of adversarial training with noisy labels (<a href="https://arxiv.org/pdf/2102.03482.pdf" target="_blank">arXiv'21</a>, <a href="https://arxiv.org/pdf/2105.14676.pdf" target="_blank">arXiv'21</a>)</p></li>
<li><p>adversarial robustness through the lens of causality (<a href="https://openreview.net/pdf?id=cZAi1yWpiXQ" target="_blank">ICLR'22</a>)</p></li>
<li><p>reliable adversarial distillation with unreliable teachers (<a href="https://openreview.net/pdf?id=u6TRGdzhfip" target="_blank">ICLR'22</a>)</p></li>
<li><p>understanding and improving graph injection attack by promoting unnoticeability (<a href="https://openreview.net/pdf?id=wkMG8cdvh7-" target="_blank">ICLR'22</a>)</p></li>
</ul>

<ul>
<h3>Automated and Federated Machine Learning</h3>
Motivated by the success of automated machine learning (AutoML), we are exploring to leverage the power of AutoML for addressing the domain problems in trustworthy learning, such as searching the small-loss percentage under noisy labels or robust network structures under adversarial examples. In high level, we have formulated the synertistic interaction between trustworthy learning and automated learning as a bi-level programming. Specifically, we designed a domain-specific search space based on domain knowledge in trustworthy learning. Meanwhile, we proposed a novel Newton algorithm to solve the bi-level optimization problem efficiently. Motivated by the success of federated learning (FL), we are exploring to leverage the power of FL for addressing the data privacy and governance issues, meanwhile maintains the model robustness to noisy labels and adversarial attacks. Besides, in industrial-level FL environments, we are the first to study the collaboration between the device and the cloud, namely the device-cloud collaborative learning (DCCL) framework.
</ul>

<ul>
Relevant Work/Publications:
</ul>
<ul>
<li><p>searching to exploit memorization effect in learning from noisy labels (<a href="https://proceedings.icml.cc/static/paper_files/icml/2020/3285-Paper.pdf" target="_blank">ICML'20</a>)</p></li>
<li><p>device-cloud collaborative learning for recommendation (<a href="https://arxiv.org/pdf/2104.06624.pdf" target="_blank">KDD'21</a>)</p></li>
<li><p>federated noisy client learning (<a href="https://arxiv.org/pdf/2106.13239.pdf" target="_blank">arXiv'21</a>)</p></li>
</ul>

<ul>
<h3>Interdisciplinary Problems: Healthcare Analytics</h3>
Unlabeled data and data with noisy labels are commonly encountered in medical image analysis. To tackle these two intractable problems, this proposed project will use machine learning (ML) technologies to develop robust, efficient and automated diagnosis algorithms, which can be applied to identify diverse diseases. We will verify our proposed methods on a series of public datasets, such as MICCAI BraTS, MICCAI iSeg2019, ChestX-ray14 and ISBI CHAOS. The aim of this project is to reduce the demands of annotated medical data, decrease the costs of manual screening, and prompt the development of smart healthcare. We hope that our designed model can provide reasonable medical interpretation for doctors, helping them better understand the functioning mechanism of intelligent medical diagnosis.
</ul>

<ul>
Relevant Projects/Publications:
</ul>
<ul>
<li><p>robust representation learning for computer-aided diagnosis (<a href="https://research.hkbu.edu.hk/page/detail/575" target="_blank">Project'20</a>)
</p></li>
<li><p>	
known-region-aware domain alignment for open world semantic segmentation (<a href="https://arxiv.org/pdf/2106.06237.pdf" target="_blank">arXiv'21</a>)
</p></li>
</ul>

</div>

   <div>
    <h2><hr><a name="sponsors"></a>Sponsors</h2>
	<ul>TML group is/was funded from Research Grants Council (RGC) of Hong Kong, National Natural Science Foundation of China (NSFC), Hong Kong Baptist University (HKBU), RIKEN Center for Advanced Intelligence Project, HKBU Computer Science Department, HKBU Interdisciplinary Research Labs, International Research Center for Neurointelligence, and Industrial Research Labs.</ul>
    <img src="aip-beta.jpg" alt="RIKEN-AIP" />
    <img src="logo-ircn-beta.jpg" alt="International Research Center for Neurointelligence" />
    </div>

</td>
</tr>
</table>
</body>
</html>
