<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>Bo Han</title>
    <base href="https://bhanML.github.io/research.html">
</head>

<body>
<h1 style="padding-left: 0.5em">Bo Han</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="index.html">Home</a></div>
	<div class="menu-item"><a href="news.html">News</a></div>
    <div class="menu-item"><a href="research.html" class="current">Research</a></div>
	<div class="menu-item"><a href="group.html">Research Group</a></div>
	<div class="menu-item"><a href="service.html">Professional Service</a></div>
    <div class="menu-item"><a href="teaching.html">Teaching</a></div>
    <div class="menu-item"><a href="codedata.html">Codes & Data</a></div>
</td>
<td id="layout-content">

<div>
        <h2><a name="research"></a>Research (Selected Topics)</h2> <br>
<ul>
<h3>Weakly Supervised Representation Learning</h3>
Modern machine learning is migrating to the era of complex models (e.g., deep neural networks), which emphasizes the data representation highly. This learning paradigm is known as representation learning. It is noted that representation learning normally requires a plethora of well-annotated data. Nonetheless, for startups or non-profit organizations, such data is barely acquirable due to the cost of labeling data or the intrinsic scarcity in the given domain. These practical issues motivate us to research and pay attention to weakly supervised representation learning (WSRL), since WSRL does not require such a huge amount of annotated data. Over the years, we have developed techniques for weakly supervised representation learning, such as label-noise representation learning and wildly transferable representation learning.
</ul>

<ul>
Relevant Work/Publications:
</ul>
<ul>
<li><p>machine learning with noisy labels (<a href="https://mitpress.mit.edu/books/series/adaptive-computation-and-machine-learning-series" target="_blank">Monograph</a>, accepted and under preparation)</p></li>
<li><p>label-noise representation learning (<a href="https://arxiv.org/pdf/2011.04406.pdf" target="_blank">Survey</a>)</p></li>
<li><p>training deep networks via memorization effects (<a href="https://proceedings.neurips.cc/paper/2018/file/a19744e268754fb0148b017647355b7b-Paper.pdf" target="_blank">NeurIPS'18</a>, <a href="http://proceedings.mlr.press/v97/yu19b/yu19b.pdf" target="_blank">ICML'19</a>, <a href="https://openreview.net/pdf?id=Eql5b1_hTE4" target="_blank">ICLR'21</a>, <a href="https://proceedings.neurips.cc/paper/2021/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf" target="_blank">NeurIPS'21</a>, <a href="https://openreview.net/pdf?id=qzM1Tw5i7N" target="_blank">TMLR'22</a>, <a href="https://openreview.net/pdf?id=xENf4QUL4LW" target="_blank">ICLR'22</a>)</p></li>
<li><p>estimating the noise transition matrix (<a href="https://proceedings.neurips.cc/paper/2018/file/aee92f16efd522b9326c25cc3237ac15-Paper.pdf" target="_blank">NeurIPS'18</a>, <a href="https://proceedings.neurips.cc/paper/2019/file/9308b0d6e5898366a4a986bc33f3d3e7-Paper.pdf" target="_blank">NeurIPS'19</a>, <a href="https://proceedings.neurips.cc/paper/2020/file/512c5cad6c37edb98ae91c8a76c3a291-Paper.pdf" target="_blank">NeurIPS'20</a>, <a href="http://proceedings.mlr.press/v139/li21l/li21l.pdf" target="_blank">ICML'21</a>, <a href="https://proceedings.mlr.press/v162/yang22p/yang22p.pdf" target="_blank">ICML'22</a>)</p></li>
<li><p>stochastic integrated gradient underweighted ascent (<a href="https://proceedings.mlr.press/v119/han20c/han20c.pdf" target="_blank">ICML'20</a>)</p></li>
<li><p>instance-dependent label-noise learning (<a href="https://proceedings.neurips.cc/paper/2020/file/5607fe8879e4fd269e88387e8cb30b7e-Paper.pdf" target="_blank">NeurIPS'20</a>, <a href="http://proceedings.mlr.press/v139/berthon21a/berthon21a.pdf" target="_blank">ICML'21</a>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17221/17028" target="_blank">AAAI'21</a>, <a href="https://proceedings.neurips.cc/paper/2021/file/23451391cd1399019fa0421129066bc6-Paper.pdf" target="_blank">NeurIPS'21</a>, <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cheng_Instance-Dependent_Label-Noise_Learning_With_Manifold-Regularized_Transition_Matrix_Estimation_CVPR_2022_paper.pdf" target="_blank">CVPR'22</a>, <a href="https://openreview.net/pdf?id=s-pcpETLpY" target="_blank">CLeaR'22</a>)</p></li>
<li><p>instance-dependent positive-unlabeled learning (<a href="https://www.computer.org/csdl/journal/tp/5555/01/09361303/1rsezbEXNxS" target="_blank">TPAMI'21</a>, <a href="https://openreview.net/pdf?id=aYAA-XHKyk" target="_blank">ICLR'22</a>)</p></li>
<li><p>class-wise denoising for robust learning under label noise (<a href="https://ieeexplore.ieee.org/document/9784878" target="_blank">TPAMI'22</a>)</p></li>
<li><p>learning with complementary and partial labels (<a href="http://proceedings.mlr.press/v119/feng20a/feng20a.pdf" target="_blank">ICML'20</a>, <a href="https://proceedings.neurips.cc/paper/2020/file/7bd28f15a49d5e5848d6ec70e584e625-Paper.pdf" target="_blank">NeurIPS'20</a>, <a href="https://openreview.net/pdf?id=qqdXHUGec9h" target="_blank">ICLR'22</a>)</p></li>
<li><p>learning with open-set labels (<a href="https://proceedings.neurips.cc/paper/2021/file/e06f967fb0d355592be4e7674fa31d26-Paper.pdf" target="_blank">NeurIPS'21</a>, <a href="https://arxiv.org/pdf/2012.00932.pdf" target="_blank">TPAMI'22</a>)</p></li>
<li><p>learning with weakly supervised labels (<a href="https://www.ijcai.org/proceedings/2020/0361.pdf" target="_blank">IJCAI'20</a>, <a href="http://proceedings.mlr.press/v139/feng21d/feng21d.pdf" target="_blank">ICML'21</a>, <a href="http://proceedings.mlr.press/v139/wu21f/wu21f.pdf" target="_blank">ICML'21</a>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17222/17029" target="_blank">AAAI'21</a>)</p></li>
<li><p>learning from crowds (<a href="https://link.springer.com/article/10.1007/s10994-017-5674-0" target="_blank">MLJ'17</a>, <a href="https://link.springer.com/article/10.1007/s10994-018-5716-2" target="_blank">MLJ'18</a>, <a href="https://link.springer.com/article/10.1007/s10994-018-5766-5" target="_blank">MLJ'18</a>)</p></li>
<li><p>variational imitation learning with diverse-quality demonstrations (<a href="https://proceedings.mlr.press/v119/tangkaratt20a/tangkaratt20a.pdf" target="_blank">ICML'20</a>)</p></li>
<li><p>contrastive learning with boosted memorization (<a href="https://proceedings.mlr.press/v162/zhou22l/zhou22l.pdf" target="_blank">ICML'22</a>)</p></li>
</ul>

<ul>
<h3>Security, Privacy and Robustness in Machine Learning</h3>
In this research thrust, I am interested in the following question: How can we preserve the security, privacy and robustness in training complex models? We have investigated learning algorithms for handling large-scale sensitive data safely. One of the key ideas is to bridge private updates of the primal variable with gradual curriculum learning. We have proposed one of the pioneer approaches for investigating the robustness of residual networks from the perspective of dynamic system. Specifically, we exploited the step factor in the Euler method to control the robustness of ResNet in both its training and generalization. More recently, we derived a series of adversarial learning algorithms, which mainly focus on empirical defense.
</ul>

<ul>
Relevant Work/Publications:
</ul>
<ul>
<li><p>privacy-preserving stochastic gradual learning (<a href="https://ieeexplore.ieee.org/document/8949708" target="_blank">TKDE'19</a>)</p></li>
<li><p>towards robust ResNet: a small step but a giant leap (<a href="https://www.ijcai.org/proceedings/2019/0595.pdf" target="_blank">IJCAI'19</a>)</p></li>
<li><p>friendly adversarial training (<a href="https://proceedings.mlr.press/v119/zhang20z/zhang20z.pdf" target="_blank">ICML'20</a>)</p></li>
<li><p>geometry-aware instance-reweighted adversarial training (<a href="https://openreview.net/pdf?id=iAX0l6Cz8ub" target="_blank">ICLR'21</a>, <a href="https://proceedings.neurips.cc/paper/2021/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf" target="_blank">NeurIPS'21</a>)</p></li>
<li><p>understanding adversarial attacks via maximum mean discrepancy (<a href="http://proceedings.mlr.press/v139/gao21b/gao21b.pdf" target="_blank">ICML'21</a>)</p></li>
<li><p>learning diverse-structured networks for adversarial robustness (<a href="http://proceedings.mlr.press/v139/du21f/du21f.pdf" target="_blank">ICML'21</a>)</p></li>
<li><p>improving adversarial robustness via invariant features and mutual information (<a href="http://proceedings.mlr.press/v139/zhou21e/zhou21e.pdf" target="_blank">ICML'21</a>, <a href="https://proceedings.mlr.press/v162/zhou22j/zhou22j.pdf" target="_blank">ICML'22</a>)</p></li>
<li><p>understanding the interaction of adversarial training with noisy labels (<a href="https://arxiv.org/pdf/2102.03482.pdf" target="_blank">arXiv'21</a>, <a href="https://openreview.net/pdf?id=zlQXV7xtZs" target="_blank">TMLR'22</a>)</p></li>
<li><p>adversarial robustness through the lens of causality (<a href="https://openreview.net/pdf?id=cZAi1yWpiXQ" target="_blank">ICLR'22</a>)</p></li>
<li><p>reliable adversarial distillation with unreliable teachers (<a href="https://openreview.net/pdf?id=u6TRGdzhfip" target="_blank">ICLR'22</a>)</p></li>
<li><p>fast and reliable evaluation of adversarial robustness (<a href="https://proceedings.mlr.press/v162/gao22i/gao22i.pdf" target="_blank">ICML'22</a>)</p></li>
<li><p>modeling adversarial noise for adversarial training (<a href="https://proceedings.mlr.press/v162/zhou22k/zhou22k.pdf" target="_blank">ICML'22</a>)</p></li>
<li><p>understanding robust overfitting (<a href="https://proceedings.mlr.press/v162/yu22b/yu22b.pdf" target="_blank">ICML'22</a>, <a href="https://arxiv.org/pdf/2205.14826.pdf" target="_blank">IJCAI'22</a>)</p></li>
<li><p>bilateral dependency optimization for privacy preservation (<a href="https://dl.acm.org/doi/pdf/10.1145/3534678.3539376" target="_blank">KDD'22</a>)</p></li>
</ul>

<ul>
<h3>Automated, Federated and Graph Machine Learning</h3>
Motivated by the success of automated machine learning (AutoML), we are exploring to leverage the power of AutoML for addressing the domain problems in trustworthy learning, such as searching the small-loss percentage under noisy labels or robust network structures under adversarial examples. In high level, we have formulated the synertistic interaction between trustworthy learning and automated learning as a bi-level programming. Specifically, we designed a domain-specific search space based on domain knowledge in trustworthy learning. Meanwhile, we proposed a novel Newton algorithm to solve the bi-level optimization problem efficiently. Motivated by the success of federated learning (FL), we are exploring to leverage the power of FL for addressing the data privacy and governance issues, meanwhile maintains the model robustness to noisy labels and adversarial attacks. Besides, in industrial-level FL environments, we are the first to study the collaboration between the device and the cloud, namely the device-cloud collaborative learning (DCCL) framework. More recently, we are working on trustworthy graph neural networks and knowledge graphs.
</ul>

<ul>
Relevant Work/Publications:
</ul>
<ul>
<li><p>searching to exploit memorization effect in learning from noisy labels (<a href="https://proceedings.mlr.press/v119/yao20b/yao20b.pdf" target="_blank">ICML'20</a>)</p></li>
<li><p>device-cloud collaborative learning for recommendation (<a href="https://dl.acm.org/doi/abs/10.1145/3447548.3467097" target="_blank">KDD'21</a>)</p></li>
<li><p>virtual homogeneity learning for federated heterogeneity (<a href="https://proceedings.mlr.press/v162/tang22d/tang22d.pdf" target="_blank">ICML'22</a>)</p></li>
<li><p>understanding and improving graph injection attack by promoting unnoticeability (<a href="https://openreview.net/pdf?id=wkMG8cdvh7-" target="_blank">ICLR'22</a>)</p></li>
<li><p>device-cloud collaborative recommendation via meta controller (<a href="https://dl.acm.org/doi/pdf/10.1145/3534678.3539181" target="_blank">KDD'22</a>)</p></li>
<li><p>efficient two-stage evolutionary architecture search (<a href="https://arxiv.org/pdf/2111.15097.pdf" target="_blank">ECCV'22</a>)</p></li>
</ul>

<ul>
<h3>Interdisciplinary Problems: Healthcare Analytics and Drug Discovery</h3>
Unlabeled data and data with noisy labels are commonly encountered in medical image analysis. To tackle these two intractable problems, this proposed project will use machine learning (ML) technologies to develop robust, efficient and automated diagnosis algorithms, which can be applied to identify diverse diseases. We will verify our proposed methods on a series of public datasets, such as MICCAI BraTS, MICCAI iSeg2019, ChestX-ray14 and ISBI CHAOS. The aim of this project is to reduce the demands of annotated medical data, decrease the costs of manual screening, and prompt the development of smart healthcare. We hope that our designed model can provide reasonable medical interpretation for doctors, helping them better understand the functioning mechanism of intelligent medical diagnosis. More recently, we are working on the synergy between machine learning and drug discovery.
</ul>

<ul>
Relevant Projects/Publications:
</ul>
<ul>
<li><p>robust representation learning for computer-aided diagnosis (<a href="https://research.hkbu.edu.hk/page/detail/575" target="_blank">Project'20</a>)
</p></li>
<li><p>	
known-region-aware domain alignment for open world semantic segmentation (<a href="https://arxiv.org/pdf/2106.06237.pdf" target="_blank">arXiv'21</a>)
</p></li>
<li><p>towards reliable, robust and invariant deep graph learning with application to drug discovery (<a href="https://www.withzz.com/project/detail/158?lang=en" target="_blank">Project'22</a>)
</p></li>
<li><p>
invariance principle meets out-of-distribution generalization on graphs (<a href="https://arxiv.org/pdf/2202.05441.pdf" target="_blank">arXiv'22</a>)
</p></li>
</ul>

</div>

   <div>
    <h2><hr><a name="sponsors"></a>Sponsors</h2>
	<ul>TMLR group is/was funded from Research Grants Council (RGC) of Hong Kong, National Natural Science Foundation of China (NSFC), Hong Kong Baptist University (HKBU), RIKEN Center for Advanced Intelligence Project, HKBU Computer Science Department, HKBU Interdisciplinary Research Labs, and Industrial Research Labs.</ul>
	<img class="center" src="rgc.jpg" alt="RGC" width="180" height="50"/>
	<img class="center" src="ugc.jpg" alt="UGC" width="160" height="50"/>
	<img class="center" src="aip-beta.jpg" alt="RIKEN AIP" width="160" height="50"/>
	<img class="center" src="microsoft.jpg" alt="Microsoft Research" width="160" height="50"/>
	<img class="center" src="nvidia.png" alt="Nvidia Research" width="160" height="50"/>
	<img class="center" src="alibaba.jpg" alt="Alibaba Research" width="150" height="50"/>
	<img class="center" src="tencentai.png" alt="Tencent AI Lab" width="70" height="50"/>
    </div>

</td>
</tr>
</table>
</body>
</html>
